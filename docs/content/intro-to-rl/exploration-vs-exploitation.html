
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Exploration and Exploitation &#8212; The Reinforcement Learning Playground</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://kit.fontawesome.com/26eda3309b.js"></script>
    <link rel="canonical" href="https://lars-quaedvlieg.github.io/RL-Playground/content/intro-to-rl/exploration-vs-exploitation.html" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Value Function Approximation" href="../deep-rl/value-function-approximation.html" />
    <link rel="prev" title="Integrating Learning and Planning" href="integrating-learning-planning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">The Reinforcement Learning Playground</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../welcome.html">
   The Reinforcement Learning Playground
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../credits.html">
   Credits
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Fundamentals of Reinforcement Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="the-rl-problem.html">
   The Reinforcement Learning Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markov-decision-processes.html">
   Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="planning-dynamic-programming.html">
   Planning by Dynamic Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model-free-prediction.html">
   Model-Free Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model-free-control.html">
   Model-Free Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="integrating-learning-planning.html">
   Integrating Learning and Planning
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Exploration and Exploitation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Value-Based Deep Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-rl/value-function-approximation.html">
   Value Function Approximation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-rl/dqn.html">
   Deep Q-Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-rl/ddqn.html">
   Double Deep Q-Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Policy Gradient Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../pg-algorithms/pg-methods.html">
   Policy Gradient Methods
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Multiagent Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../marl/rl-classic-games.html">
   Reinforcement Learning in Classic Games
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a target="_blank" href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/intro-to-rl/exploration-vs-exploitation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-armed-bandits">
   Multi-Armed Bandits
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#upper-confidence-bounds">
     Upper Confidence Bounds
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-bandits">
     Bayesian Bandits
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability-matching">
     Probability Matching
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#information-state-search">
     Information State Search
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contextual-bandits">
   Contextual Bandits
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#markov-decision-processes">
   Markov Decision Processes
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Exploration and Exploitation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-armed-bandits">
   Multi-Armed Bandits
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#upper-confidence-bounds">
     Upper Confidence Bounds
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-bandits">
     Bayesian Bandits
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability-matching">
     Probability Matching
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#information-state-search">
     Information State Search
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contextual-bandits">
   Contextual Bandits
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#markov-decision-processes">
   Markov Decision Processes
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="exploration-and-exploitation">
<h1>Exploration and Exploitation<a class="headerlink" href="#exploration-and-exploitation" title="Permalink to this headline">¶</a></h1>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Please note that this notebook is still <strong>work in progress</strong>, hence some sections could behave unexpectedly, or provide incorrect information.</p>
</div>
<p>The trade-off between exploration and exploitation is a fundamental problem in Reinforcement Learning and online
decision-making. It provides a trade-off between making a decision based on the current beliefs, or gather more of this
information.</p>
<p>In general, the long-term is preferred over short-turn. This means the best strategy might include short-term sacrifices
. The foal is to gather enough information to make the best overall decisions.</p>
<p>This chapter will first explain exploration in a special forms of an MDP, called bandit problems. Finally, these ideas
are generalized to the general MDP.</p>
<p>There are lots of <em>principles</em> to make sure to explore. There include but are not limited to the following</p>
<ul class="simple">
<li><p><strong>Naive Exploration</strong>: Add noise to greedy policy (e.g. <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy)</p></li>
<li><p><strong>Optimistic Initialization</strong>: Assume the best until proven otherwise</p></li>
<li><p><strong>Optimism in the face of uncertainty</strong>: Prefer actions with uncertain values</p></li>
<li><p><strong>Probability Matching</strong>: Select actions based on probability their are best</p></li>
<li><p><strong>Information State Search</strong>: Look-ahead search incorporating value of information</p></li>
</ul>
<div class="section" id="multi-armed-bandits">
<h2>Multi-Armed Bandits<a class="headerlink" href="#multi-armed-bandits" title="Permalink to this headline">¶</a></h2>
<p>A multi-armed bandit is a tuple <span class="math notranslate nohighlight">\((A, R)\)</span>. Here, <span class="math notranslate nohighlight">\(A\)</span> is a known set of actions (arms) and
<span class="math notranslate nohighlight">\(R^a(r) = \mathbb{P}\left[r | a\right]\)</span> is an unknown probability distribution over rewards. At each step <span class="math notranslate nohighlight">\(t\)</span>, the agent
chooses an action <span class="math notranslate nohighlight">\(a_t \in A\)</span>. Then, the environment generates a reward <span class="math notranslate nohighlight">\(r_t \sim R^{a_t}\)</span>. The goal is to maximize the
cumulative reward <span class="math notranslate nohighlight">\(\sum_{T = 1}^{t} r_T\)</span>.</p>
<p>The goal is to maximize this reward. However, this can otherwise be states as <em>minimizing</em> the total
<strong>regret</strong>. The action-value is the expected reward of an action (<span class="math notranslate nohighlight">\(Q(a) = \mathbb{E}\left[r | a\right]\)</span>). The optimal value
<span class="math notranslate nohighlight">\(V* = Q(a^*) = \max_{a \in A} Q(a)\)</span>.</p>
<p>The regret is the measured opportunity loss for one step: <span class="math notranslate nohighlight">\(l_t = \mathbb{E}\left[V^* - Q(a_t)\right]\)</span>. The <em>total regret</em>
is then equal to the total opportunity loss <span class="math notranslate nohighlight">\(L_t = \mathbb{E}\left[\sum_{T = 1}^{t} V^* - Q(a_t)\right]\)</span>.</p>
<p>It is possible to count regret. The <em>count</em> <span class="math notranslate nohighlight">\(N_t(a)\)</span> is the expected number of selection for action <span class="math notranslate nohighlight">\(a\)</span>. For that
action, there is a <em>gap</em> in difference of value between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(a^*\)</span>. This gap equals <span class="math notranslate nohighlight">\(\Delta_a = V^* - Q(a)\)</span>.
The regret can then be rewritten to the following</p>
<div class="math notranslate nohighlight">
\[\begin{split}		L_t &amp; = \mathbb{E}\left[\sum_{T = 1}^{t} V^* - Q(a_t)\right]\\
		    &amp; = \sum_{a \in A} \mathbb{E}\left[N_t(a)\right] (V^* - Q(a_t))\\
		    &amp; = \sum_{a \in A} \mathbb{E}\left[N_t(a)\right] \Delta_a\end{split}\]</div>
<p>So, the regret is a function of the gaps and counts. A good algorithm will ensure small counts for large gaps. There is
a problem however; the gaps are not known.</p>
<div class="figure align-default" id="figure-regret-over-time">
<a class="reference internal image-reference" href="../../_images/regret-over-time.png"><img alt="../../_images/regret-over-time.png" src="../../_images/regret-over-time.png" style="width: 14cm;" /></a>
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text">Total regret over time</span><a class="headerlink" href="#figure-regret-over-time" title="Permalink to this image">¶</a></p>
</div>
<p>From <a class="reference internal" href="#figure-regret-over-time"><span class="std std-numref">Fig. 9</span></a>, the following observations can be made. If an algorithm <em>never</em>- or
<em>forever</em> explores, the total regret will be <em>linear</em>. This is not good, since the goal is to minimize
regret while exploring. The idea of <em>decaying</em> <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy is interesting, since it will converge to a
logarithmic regret.</p>
<p>Lets estimate <span class="math notranslate nohighlight">\(Q(a) \approx \hat{Q}_t(a)\)</span>. The value of each action can be evaluated by MC-evaluation:
<span class="math notranslate nohighlight">\(\hat{Q}_t(a) = \frac{1}{N_t(a)} \sum_{t = 1}^{T} r_t 1(a_t = a)\)</span>.</p>
<p>Since the <em>greedy</em> algorithm always selects the action with the highest value, it can lock onto a sub-optimal
action forever, thus receiving a linear total regret.</p>
<p>Using an <em><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy</em> algorithm, the constant <span class="math notranslate nohighlight">\(\epsilon\)</span> ensures that the minimum regret
<span class="math notranslate nohighlight">\(l_t \geq \frac{\epsilon}{A} \sum_{a \in A} \Delta_a\)</span>. Thus, the total regret is also linear.</p>
<p>A simple but powerful idea is to use <strong>Optimistic Initialization</strong>. Initialize <span class="math notranslate nohighlight">\(Q(a)\)</span> to a very high value and from
there on update the action-value by MC evaluation starting with <span class="math notranslate nohighlight">\(N(a) &gt; 0\)</span>. It encourages exploration early on, but can
still lock onto sub-optimal actions. In practice however, this generally works really well.</p>
<p>The <em>decaying <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy</em> algorithm works by picking a decaying schedule for <span class="math notranslate nohighlight">\(\epsilon_1, \epsilon_2, ...\)</span>
. Imagine the following schedule</p>
<div class="math notranslate nohighlight">
\[\begin{split}    c &amp; &gt; 0\\
    d &amp; = \min_{a | \Delta_a &gt; 0} \Delta_i\\
    \epsilon_t &amp; = \min \{1, \frac{c|A|}{d^2t}\}\end{split}\]</div>
<p>There is a problem with the schedule though. It uses advance knowledge about gaps. So, the goal is to find an algorithm
with sub-linear regret for any multi-armed bandit without using knowledge of <span class="math notranslate nohighlight">\(R\)</span>.</p>
<p>The performance of any algorithm is determined by the <em>similarity</em> between the optimal arm and other arms. Hard
problem have similar-looking arms with different means. This can be described as the size of the gap, and the similarity
in distributions (measure by KL-divergence).</p>
<p>The following <strong>Lower Bound</strong> is a theorem. It means the <em>asymptotic total regret is at least logarithmic in
the number of steps</em>.</p>
<div class="math notranslate nohighlight">
\[
	\lim_{t \rightarrow \infty} L_t \geq \log t \sum_{a | \Delta_a &gt; 0} \frac{\Delta_a}{KL(R^a||R^{a^*})}
\]</div>
<div class="section" id="upper-confidence-bounds">
<h3>Upper Confidence Bounds<a class="headerlink" href="#upper-confidence-bounds" title="Permalink to this headline">¶</a></h3>
<div class="figure align-default">
<a class="reference internal image-reference" href="../../_images/oitfou-before.png"><img alt="../../_images/oitfou-before.png" src="../../_images/oitfou-before.png" style="width: 14cm;" /></a>
</div>
<p>Take a look at the figure above. The idea of <strong>Optimism in the Face of Uncertainty</strong> is to pick the action with
the highest potential. This would be <span class="math notranslate nohighlight">\(a_1\)</span>, since the tail of the distribution is the best one of them all. Say you pick
this action. Then, the distribution gets updated.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../../_images/oitfou-after.png"><img alt="../../_images/oitfou-after.png" src="../../_images/oitfou-after.png" style="width: 14cm;" /></a>
</div>
<p>Now, we are more certain about the actual value of <span class="math notranslate nohighlight">\(Q(a_1)\)</span> and thus more likely to pick another action until the
optimal one has been discovered.</p>
<p>Another way to estimate uncertainty is through <strong>Upper Confidence Bounds</strong> (UCB). An upper confidence
<span class="math notranslate nohighlight">\(\hat{U}_t(a)\)</span> can be calculated for each action-value, such that <span class="math notranslate nohighlight">\(Q(a) \leq \hat{Q}_t(a) + \hat{U}_t(a)\)</span> with high
probability. The action maximizing the UCB is then selected.</p>
<p>By <strong>Hoeffding’s Inequality</strong>, <span class="math notranslate nohighlight">\(\mathbb{P}\left[\mathbb{E}\left[X\right] &gt; \bar{X}_t + u\right] \leq e^{-2tu^2}\)</span> for
<span class="math notranslate nohighlight">\(\bar{X}_t = \frac{1}{T} \sum_{t = 1}^T X_t\)</span>, where <span class="math notranslate nohighlight">\(X_t\)</span> is an i.i.d. r.v. in <span class="math notranslate nohighlight">\([0, 1]\)</span>. When applying this to the
bandit problem, the following can be derived</p>
<div class="math notranslate nohighlight">
\[
	\mathbb{P}\left[Q(a) &gt; \hat{Q}_t(a) + U_t(a)\right] \leq e^{-2N_t(a)U_t(a)^2}
\]</div>
<p>Now, pick a probability <span class="math notranslate nohighlight">\(p\)</span> that the true value exceeds the UCB.</p>
<div class="math notranslate nohighlight">
\[\begin{split}    p &amp; = e^{-2N_t(a)U_t(a)^2}\\
    U_t(a) &amp; = \sqrt{\frac{-\log p}{2 N_t(a)}}\end{split}\]</div>
<p>Since the goal is to reduce the probability as more rewards are observed, set e.g. <span class="math notranslate nohighlight">\(p = t^{-4}\)</span>. Then ensures the
optimal action is selected when <span class="math notranslate nohighlight">\(t \rightarrow \infty\)</span>. The final equation becomes</p>
<div class="math notranslate nohighlight">
\[
	U_t(a) = \sqrt{\frac{2 \log t}{N_t(a)}}
\]</div>
<p>This leads to the <strong>UCB1</strong> algorithm</p>
<div class="math notranslate nohighlight">
\[
	a_t = \arg\max_{a \in A} Q(a) + \sqrt{\frac{2 \log t}{N_t(a)}}
\]</div>
<p><em>Theorem</em>: the UCB algorithm achieves logarithmic asymptotic total regret</p>
<div class="math notranslate nohighlight">
\[
	\lim_{t \rightarrow \infty} L_t \leq 8 \log t \sum_{a | \Delta_a &gt; 0} \Delta_a
\]</div>
</div>
<div class="section" id="bayesian-bandits">
<h3>Bayesian Bandits<a class="headerlink" href="#bayesian-bandits" title="Permalink to this headline">¶</a></h3>
<p>So far, no assumptions were made about the reward distribution <span class="math notranslate nohighlight">\(R\)</span>, except for bounds on rewards.
<strong>Bayesian Bandits</strong> exploit prior knowledge of rewards, <span class="math notranslate nohighlight">\(p\left[R\right]\)</span>. They compute the posterior distribution
of reward <span class="math notranslate nohighlight">\(p\left[R | h_t\right]\)</span>, where <span class="math notranslate nohighlight">\(h_t = a_1, r_1, ..., a_{t-1}, r_{t-1}\)</span> is the history. The posterior
distribution can then be used to guide exploration.</p>
<p>The following is an example. Assume that the reward distribution is Gaussian, so <span class="math notranslate nohighlight">\(R_a(r) = N(r; \mu_a, \sigma_a^2)\)</span>. The
Gaussian posterior over <span class="math notranslate nohighlight">\(\mu_a\)</span> and <span class="math notranslate nohighlight">\(\sigma_a^2\)</span> can be computed using Bayes’ law:
<span class="math notranslate nohighlight">\(p[\mu_a, \sigma_a^2; h_t] \propto p[\mu_a, \sigma_a^2] \prod_{t | a_t = a} N(r_t; \mu_a, \sigma_a^2)\)</span>.</p>
<p>Then, pick the action that maximizes the standard deviation of <span class="math notranslate nohighlight">\(Q(a)\)</span>, so
<span class="math notranslate nohighlight">\(a_t = \arg\max_{a \in A} \mu_a + \frac{c \sigma_a}{\sqrt{N(a)}}\)</span>. This is a Bayesian way to compute Upper Confidence
Bounds.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../../_images/ucb-bayesian.png"><img alt="../../_images/ucb-bayesian.png" src="../../_images/ucb-bayesian.png" style="width: 14cm;" /></a>
</div>
<p>Here, the blue action would be taken. This is because the mean and standard deviation summed up is the largest
(assuming an equal number of visits).</p>
</div>
<div class="section" id="probability-matching">
<h3>Probability Matching<a class="headerlink" href="#probability-matching" title="Permalink to this headline">¶</a></h3>
<p><strong>Probability Matching</strong> selects an action <span class="math notranslate nohighlight">\(a\)</span> according to the probability that <span class="math notranslate nohighlight">\(a\)</span> is optimal. It can be
calculated using</p>
<div class="math notranslate nohighlight">
\[
	\pi(a) = \mathbb{P} \left[Q(a) = \max_{a'} Q(a') | R_1, ..., R_{t-1}\right]
\]</div>
<p>This concept is optimistic in the face of uncertainty, since uncertain actions have a higher probability of being the
maximum. A disadvantage however is that it can be difficult to be computed analytically from the posterior.</p>
<p><strong>Thompson Sampling</strong> is sample-based probability matching. The policy can be computed using</p>
<div class="math notranslate nohighlight">
\[
	\pi(a) = \mathbb{E}\left[1(Q(a) = \max_{a'} Q(a')) | R_1, ..., R_{t-1}\right]
\]</div>
<p>Then, use Bayes’ law to compute the posterior distribution <span class="math notranslate nohighlight">\(P_w(Q|R_1, ..., R_{t-1})\)</span>. First sample an action-value
<span class="math notranslate nohighlight">\(Q(a)\)</span> from the posterior. Then, choose action <span class="math notranslate nohighlight">\(A_t = \arg\max_{a \in A} Q(a)\)</span>.</p>
</div>
<div class="section" id="information-state-search">
<h3>Information State Search<a class="headerlink" href="#information-state-search" title="Permalink to this headline">¶</a></h3>
<p>The goal of exploration is to gain more information about the environment. If it is possible to quantify how much this
information is worth. When this value is known, the optimal trade-off between exploration and exploitation is known.</p>
<p>The idea is to look at bandits as <em>sequential</em> decision-making processes instead of <em>one-step</em>. At each
step, there is an <strong>information state</strong> <span class="math notranslate nohighlight">\(\bar{s} = f(h_t)\)</span>. So, the state is a summary of all history that has been
accumulated so far. Each action <span class="math notranslate nohighlight">\(a\)</span> then causes a transition to a new information state <span class="math notranslate nohighlight">\(\bar{s}'\)</span>, with probability
<span class="math notranslate nohighlight">\(\bar{P}^a_{\bar{s}\bar{s}'}\)</span>.</p>
<p>This defines the MDP <span class="math notranslate nohighlight">\(\bar{M}\)</span> in augmented information state space <span class="math notranslate nohighlight">\(\bar{M} = (\bar{S}, A, \bar{P}, R, \gamma)\)</span>.</p>
<p>An example of this are Bernoulli Bandits, so lets say <span class="math notranslate nohighlight">\(R^a = Bernoulli(\mu_a)\)</span>. The goal is to find the arm with the
highest <span class="math notranslate nohighlight">\(\mu_a\)</span>. The information state space <span class="math notranslate nohighlight">\(\bar{s} = (\alpha, \beta)\)</span>, where <span class="math notranslate nohighlight">\(\alpha_a\)</span> and <span class="math notranslate nohighlight">\(\beta_b\)</span> count when the
reward was 0 or 1 respectively.	 This MDP can then be solved by previously discussed Reinforcement Learning algorithms.
There is an alternative approach, referred to as <strong>Bayes-adaptive RL</strong>. For Bernoulli bandits, it finds the
Bayes-optimal exploration/exploitation trade-off with respect to the prior distribution.</p>
<p>Imagine a problem of drug assignment. There are two drugs 1 and 2 that can be used;
<span class="math notranslate nohighlight">\(\bar{s} = (\alpha_1, \beta_1, \alpha_2, \beta_2)\)</span>. First, start with a <span class="math notranslate nohighlight">\(Beta(\alpha_a, \beta_a)\)</span> prior over reward
function <span class="math notranslate nohighlight">\(R^a\)</span>. Each time <span class="math notranslate nohighlight">\(a\)</span> is selected, the distribution gets updated accordingly, since a success will increase beta
, and a failure increases alpha. Here, each state transition corresponds to a Bayesian model update.</p>
<p>The solution of this problem is known as the <em>Gittens</em> index, and can be computed by solving the MDP (by using
for example MCTS for planning).</p>
</div>
</div>
<div class="section" id="contextual-bandits">
<h2>Contextual Bandits<a class="headerlink" href="#contextual-bandits" title="Permalink to this headline">¶</a></h2>
<p>A <strong>Contextual Bandit</strong> is a tuple <span class="math notranslate nohighlight">\((A, S, R)\)</span>. Instead of the bandit problem, there is now also a state space,
which provides <em>context</em> to the problem. <span class="math notranslate nohighlight">\(S = \mathbb{P}[s]\)</span> is an unknown distribution over the contexts. Then, the
reward <span class="math notranslate nohighlight">\(R^a_s(r) = \mathbb{P}[r|s, a]\)</span> is an unknown probability distribution of the rewards. At each time step, the
environment generates <span class="math notranslate nohighlight">\(s_t \sim S\)</span>. Then, an agent selects <span class="math notranslate nohighlight">\(a_t \in A\)</span>, to which the environment generates
<span class="math notranslate nohighlight">\(r_t \sim R^{a_t}_{s_t}\)</span>. The goal is to maximize <span class="math notranslate nohighlight">\(\sum_{T=1}^t r_T\)</span>.</p>
<p>This section explains the concept of <strong>Linear UCB</strong>. The action-value function needs to be estimated:
<span class="math notranslate nohighlight">\(Q(s, a) = \mathbb{E}[r|s,a] \approx Q_\theta(s, a) = x(s, a)^\intercal \theta\)</span>. The parameters of this linear function can be
estimated by least-squares. The solution is</p>
<div class="math notranslate nohighlight">
\[
	\theta_t = \left(\sum_{T=1}^t x(s_T, a_T)x(s_T, a_T)^\intercal\right)^{-1} \sum_{T=1}^t x(s_T, a_T)r_T
\]</div>
<p>Least squares regression estimates <span class="math notranslate nohighlight">\(Q_\theta(s, a)\)</span>. However, it can also be used to estimate the variance of the
action-value <span class="math notranslate nohighlight">\(\sigma^2_\theta(s, a)\)</span>. The idea is to add a bonus for uncertainty, <span class="math notranslate nohighlight">\(U_\theta(s, a) = c\sigma\)</span>, where the
UCB is the number of standard deviations above the mean defined by <span class="math notranslate nohighlight">\(c\)</span>.</p>
<p>For least squares regression, parameter covariance is
<span class="math notranslate nohighlight">\(A^{-1} = \left(\sum_{T=1}^t x(s_T, a_T)x(s_T, a_T)^\intercal\right)^{-1}\)</span>. The action-value is linear in features, so</p>
<div class="math notranslate nohighlight">
\[\begin{split}    \sigma^2_\theta(s, a) &amp; = x(s, a)^\intercal A^{-1} x(s, a)\\
    a_t &amp; = \arg\max_{a \in A} Q_\theta(s_t, a) + c\sigma_\theta(s_t, a)\\
    a_t &amp; = \arg\max_{a \in A} Q_\theta(s_t, a) + c \sqrt{x(s, a)^\intercal A^{-1} x(s, a)}\end{split}\]</div>
</div>
<div class="section" id="markov-decision-processes">
<h2>Markov Decision Processes<a class="headerlink" href="#markov-decision-processes" title="Permalink to this headline">¶</a></h2>
<p>All previously described principles for exploration/exploitation can apply to MDPs with slights modifications.</p>
<ul>
<li><p>Optimistic Initialization</p>
<ul>
<li><p>Model-Free RL</p>
<p>The idea is to initialize <span class="math notranslate nohighlight">\(Q(s, a)\)</span> to <span class="math notranslate nohighlight">\(\frac{r_{max}}{1 - \gamma}\)</span>. This encourages systematic exploration of
states and actions.</p>
</li>
<li><p>Model-Based RL</p>
<p>Here, an optimistic model of the MDP is constructed. For example, transitions to a terminal state are
initialized with <span class="math notranslate nohighlight">\(r_{max}\)</span> reward. An example is the RMax algorithm (Brafman and Tennenholtz).</p>
</li>
</ul>
</li>
<li><p>Optimism in the Face of Uncertainty</p>
<ul>
<li><p>Model-Free RL</p>
<p>The goal is to maximize UCB on the action-value function <span class="math notranslate nohighlight">\(Q^\pi(s, a)\)</span>.
<span class="math notranslate nohighlight">\(a_t = \arg\max_{a \in A} Q(s_t, a) + U(s_t, a)\)</span>. It estimates uncertainty in policy evaluation. However, it
completely ignores uncertainty in the policy improvement step.</p>
<p>For this reason, maximize UCB on <span class="math notranslate nohighlight">\(Q^*(s, a)\)</span> instead.
<span class="math notranslate nohighlight">\(a_t = \arg\max_{a \in A} Q(s_t, a) + U_1(s_t, a) + U_2(s_t, a)\)</span>. This estimates uncertainty in both policy
evaluation and policy improvement. However, estimating policy improvement uncertainty is hard to do.</p>
</li>
<li><p>Model-Based RL</p>
<p>Bayesian Model-Based RL maintains a posterior distribution over MDP models. Both transitions and rewards are
estimated <span class="math notranslate nohighlight">\(p[P, R| h_t], h_t = s_1, a_1, r_2, ..., s_t\)</span>. This posterior can then be used to guide exploration
by for example Bayesian UCB or Thompson Sampling.</p>
</li>
</ul>
</li>
<li><p>Probability Matching</p>
<p>Thompson sampling implements probability matching</p>
<div class="math notranslate nohighlight">
\[\begin{split}      \pi(s, a | h_t) &amp; = \mathbb{P}[Q^*(s, a) &gt; Q^*(s, a'), \forall a' \neq a | h_t]\\
                      &amp; = \mathbb{E}_{P, R| h_t}\left[1(a = \arg\max_{a \in A} Q^*(s, a))\right]\end{split}\]</div>
<p>Bayes’ Law can be used to compute posterior distribution <span class="math notranslate nohighlight">\(p[P, R| h_t]\)</span>. Then, sample an MDP <span class="math notranslate nohighlight">\(P, R\)</span> from the
posterior. Then, solve this MDP using a favored planning algorithm to obtain <span class="math notranslate nohighlight">\(Q^*(s, a)\)</span>. Then, select the optimal
action from the sample MDP by <span class="math notranslate nohighlight">\(a_t \arg\max_{a \in A} Q^*(s_t, a)\)</span>.</p>
</li>
<li><p>Information State Search</p>
<p>MDPs can also be augmented to include an information state. The augmented state becomes <span class="math notranslate nohighlight">\((s, \hat{s})\)</span>, where <span class="math notranslate nohighlight">\(s\)</span> is
the original state within the MDP, and <span class="math notranslate nohighlight">\(\hat{s}\)</span> is accumulated information about the history. Each action <span class="math notranslate nohighlight">\(a\)</span>
causes a transition to <span class="math notranslate nohighlight">\(s'\)</span> with probability <span class="math notranslate nohighlight">\(P^a_{ss'}\)</span>. However, a new information state <span class="math notranslate nohighlight">\(\hat{s}'\)</span> is also
created.</p>
<p>The posterior distribution over the MDP model is an information state <span class="math notranslate nohighlight">\(\hat{s_t} = \mathbb{P}[P, R| h_t]\)</span>. An augmented
MDP over <span class="math notranslate nohighlight">\((s, \hat{s})\)</span> is called a <strong>Bayes-Adaptive MDP</strong>.</p>
<p>Solving this MDP gives rise to the optimal exploration/exploitation trade-off. However, the Bayes-Adaptive MDP is
typically enormous. Simulation-based search has been proven effective to this problem.</p>
</li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content\intro-to-rl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="integrating-learning-planning.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Integrating Learning and Planning</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../deep-rl/value-function-approximation.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Value Function Approximation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Lars Quaedvlieg<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>